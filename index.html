<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="css/style.css">
    <!-- 
        PT Sans
        #eee2dc
        #2f4454
        #2e151b
        #da7b93
        #376ee6f
        #1c3334
     -->
</head>

<body>
    <header>
        <ul class="gnb">
            <li class="push-left"><a href="index.html">Myung Jin Kim</a></li>
            <li><a href="#">Home</a></li>
            <li><a href="#about">About</a></li>
            <li><a href="#projects">Work</a></li>
            <li class="push-right"><a href="#none">Contact</a></li>
        </ul>
    </header>

    <div class="about" id="about">
        <div class="flx-cont">
            <div class="photo">
                <img src="images/profile-picture.jpg" alt="profile photo">
            </div>
            <div class="about-me">
                <h2>Hi, I'm Myung Jin (MJ) Kim.</h2>
                <div class="paragraph">
                    <p>I am a student researcher in <strong>Human-Computer Interaction (HCI)</strong>,<br>specializing in
                        <strong>tangible interfaces</strong>, <strong>mixed reality experiences</strong>,<br> and
                        <strong>fabrication techniques</strong>.</p>
                    <br>
                    <p>Currently I am a PhD candidate at the <a href="https://makinteract.kaist.ac.kr/"
                            target="_blank">MAKinteract Lab</a><br>in the Industrial Design Department in <a
                            href="https://www.kaist.ac.kr/en/" target="_blank">KAIST.</a></p>
                    <p>I am also part of the <a href="https://hci.kaist.ac.kr/" target="_blank">HCI@KAIST</a> group.</p>
                </div>
                <div class="link">
                    <a class="bundle" href="#none" target="_blank">
                        <img src="images/cv-white.png">
                        <span>Curriculum<br>Vitae</span>
                    </a>
                    <a class="bundle" href="#none" target="_blank">
                        <img src="images/scholar-white.png">
                        <span>Google<br>Scholar</span>
                    </a>
                </div>
            </div>
        </div>
    </div>

    <div class="projects" id="projects">
        <div class="flx-cont">
            <h2>Projects</h2>

            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">SpinOcchio: Understanding Haptic-Visual Congruency of Skin-Slip in VR with a Dynamic Grip Controller (CHI '22 Full Paper)</a></h3>
                    <h4><strong>Myung Jin Kim</strong>, Neung Ryu, Wooje Chang, Michel Pahud, Mike Sinclair, and Andrea Bianchi </h4>
                    <p>
                        "This paper's goal is to understand the haptic-visual congruency perception of skin-slip on the fingertips given visual cues in Virtual Reality (VR). We developed SpinOcchio ('Spin' for the spinning mechanism used, 'Occhio' for the Italian word “eye”), a handheld haptic controller capable of rendering the thickness and slipping of a virtual object pinched between two fingers. This is achieved using a mechanism with spinning and pivoting disks that apply a tangential skin-slip movement to the fingertips. With SpinOcchio, we determined the baseline haptic discrimination threshold for skin-slip, and, using these results, we tested how haptic realism of motion and thickness is perceived with varying visual cues in VR. Surprisingly, the results show that in all cases, visual cues dominate over haptic perception. Based on these results, we suggest applications that leverage skin-slip and grip interaction, contributing further to realistic experiences in VR."
                    </p>
                </div>
            </div>
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">Exploring Pseudo Hand-Eye Interaction on the Head-Mounted Display (AHs '21)</a></h3>
                    <h4><strong>[Honorable Mention Award]</strong></h4>
                    <h4><strong>Myung Jin Kim</strong> and Andrea Bianchi </h4>
                    <p>
                        "Virtual and augmented reality devices and applications have enabled the user to experience a variety of simulated real-life experiences through first-person visual, auditory, and haptic feedback. However, among the numerous everyday interactions that have been emulated, the familiar interaction of touching or rubbing the eyes is yet to be explored and remains to be understood. In this paper, we aim to understand the components of natural hand-eye interaction, propose an interaction technique through a proof-of-concept prototype head-mounted display, and evaluate the user experience of the prototype through a user study. In addition, we share insights emerged from the studies with suggestions for further development of interaction techniques based on combinations of hardware and software. "
                    </p>
                </div>
            </div>
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object (SA '20 E-Tech)</a></h3>
                    <h4>Neung Ryu, <strong>Myung Jin Kim</strong>, and Andrea Bianchi </h4>
                    <p>
                        "We present ElaStick, a handheld variable stiffness controller capable of simulating the kinesthetic sensation of deformable and flexible objects when swung or shaken. ElaStick is capable of rendering gradual changes of stiffness along two independent axes over a wide continuous range. Two trackers on the controller enable a closed-loop feedback that allows to accurately map the device's deformations to the visuals of a Virtual Reality application."
                    </p>
                </div>
            </div>
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object (UIST '20 Full Paper)</a></h3>
                    <h4><strong>[Best Paper Award]</strong></h4>
                    <h4>Neung Ryu, Woojin Lee, <strong>Myung Jin Kim</strong>, and Andrea Bianchi </h4>
                    <p>
                        "Haptic controllers have an important role in providing rich and immersive Virtual Reality (VR) experiences. While previous works have succeeded in creating handheld devices that simulate dynamic properties of rigid objects, such as weight, shape, and movement, recreating the behavior of flexible objects with different stiffness using ungrounded controllers remains an open challenge. In this paper we present ElaStick, a variable-stiffness controller that simulates the dynamic response resulting from shaking or swinging flexible virtual objects. This is achieved by dynamically changing the stiffness of four custom elastic tendons along a joint that effectively increase and reduce the overall stiffness of a perceived object in 2-DoF. We show that with the proposed mechanism, we can render stiffness with high precision and granularity in a continuous range between 10.8 and 71.5Nmm/degree. We estimate the threshold of the human perception of stiffness with a just-noticeable difference (JND) study and investigate the levels of immersion, realism and enjoyment using a VR application."
                    </p>
                </div>
            </div>
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">BodyPrinter: Fabricating Circuits Directly on the Skin at Arbitrary Locations Using a Wearable Compact Plotter (UIST '20 Full Paper) </a></h3>
                    <h4>Youngkyung Choi, Neung Ryu, <strong>Myung Jin Kim</strong>, Artem Dementyev, and Andrea Bianchi </h4>
                    <p>
                        "On-body electronics and sensors offer the opportunity to seamlessly augment the human with computing power. Accordingly, numerous previous work investigated methods that exploit conductive materials and flexible substrates to fabricate circuits in the form of wearable devices, stretchable patches, and stickers that can be attached to the skin. For all these methods, the fabrication process involves several manual steps, such as designing the circuit in software, constructing conductive patches, and manually placing these physical patches on the body. In contrast, in this work, we propose to fabricate electronics directly on the skin. We present BodyPrinter, a wearable conductive-ink deposition machine, that prints flexible electronics directly on the body using skin-safe conductive ink. The paper describes our system in detail and, through a series of examples and a technical evaluation, we show how direct on-body fabrication of electronic circuits and sensors can further enhance the human body."
                    </p>
                </div>
            </div>           
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">Aero-plane: A Handheld Force-Feedback Device that Renders Weight Motion Illusion on a Virtual 2D Plane (UIST '19 Full Paper & Demo) </a></h3>
                    <h4>Seungwoo Je, <strong>Myung Jin Kim</strong>, Woojin Lee, Byungjoo Lee, Xing-Dong Yang, Pedro Lopes, and Andrea Bianchi </h4>
                    <p>
                        "Force feedback is said to be the next frontier in virtual reality (VR). Recently, with consumers pushing forward with untethered VR, researchers turned away from solutions based on bulky hardware (e.g., exoskeletons and robotic arms) and started exploring smaller portable or wearable devices. However, when it comes to rendering inertial forces, such as when moving a heavy object around or when interacting with objects with unique mass properties, current ungrounded force feedback devices are unable to provide quick weight shifting sensations that can realistically simulate weight changes over 2D surfaces. In this paper we introduce Aero-plane, a force-feedback handheld controller based on two miniature jet propellers that can render shifting weights of up to 14 N within 0.3 seconds. Through two user studies we: (1) characterize the users' ability to perceive and correctly recognize different motion paths on a virtual plane while using our device; and, (2) tested the level of realism and immersion of the controller when used in two VR applications (a rolling ball on a plane, and using kitchen tools of different shapes and sizes). Lastly, we present a set of applications that further explore different usage cases and alternative form-factors for our device."
                    </p>
                </div>
            </div>
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">Wind-Blaster: a wearable propeller-based prototype that provides ungrounded force-feedback (SIGGRAPH 18 E-Tech) </a></h3>
                    <h4>Seungwoo Je, Hyelip Lee, <strong>Myung Jin Kim</strong>, Andrea Bianchi</h4>
                    <p>
                        "Ungrounded haptic force-feedback is a crucial element for applications that aim to immerse users in virtual environments where also mobility is an important component of the experience, like for example Virtual Reality games. In this paper, we present a novel wearable interface that generates a force-feedback by spinning two drone-propellers mounted on a wrist. The device is interfaced with a game running in Unity, and it is capable to render different haptic stimuli mapped to four weapons. A simple evaluation with users demonstrates the feasibility of the proposed approach."
                    </p>
                </div>
            </div>           
            <hr>
            <div class="block">
                <iframe src="https://www.youtube.com/embed/yZHIUnK6DFY" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe>
                <div class="description">
                    <h3><a href="#none">An Artistic Provocation to Explore Effects and Opportunities of Virtual Surreal Spaces (DIS '18 Provocations and Work-in-Progress)</a></h3>
                    <h4>Hyelip Lee, Myung Jin Kim, Byungjoo Lee, Andrea Bianchi</h4>
                    <p>
                        "The concept of surreal virtual space is used in this paper to describe a space which looks realistic but is impossible to exist in reality. For this project, we developed a 3D virtual space using Google Cardboard and an Android mobile device. Referring to the 2D drawing, "Relativity," of M.C. Escher, the virtual space was designed to have multi-directional but connected stairs. This work was exhibited with other artworks at a gallery for a period of three weeks. Despite some minor sensory confusion, all audiences experienced a degree of place illusion, enjoyment and a sense of self-awareness even though the virtual environment did not provide a visual representation of the audience's own body. For future work, we plan to investigate the advantages of these effects and apply them to everyday non-virtual environments. "
                    </p>
                </div>
            </div>
        </div>
    </div>


</body>

</html>